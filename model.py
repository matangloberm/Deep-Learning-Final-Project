# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KirJANjv2R6OLUPRAhkjvTUABVKmD0PB
"""

import torch.nn as nn
import torch.nn.init as init
import config2 as config
import importlib




class FullyConnected(nn.Module):
    def __init__(self, input_size = config.args.input_size , hidden_size1 = config.args.hidden_size1, hidden_size2 = config.args.hidden_size2
                 , num_classes = config.args.num_classes, dropout = config.args.dropout):
        super(FullyConnected, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size1)
        self.relu1 = nn.ReLU()
        self.dropout1 = nn.Dropout(dropout)

        self.fc2 = nn.Linear(hidden_size1, hidden_size2)
        self.relu2 = nn.ReLU()
        self.dropout2 = nn.Dropout(dropout)

        self.fc3 = nn.Linear(hidden_size2, num_classes)

        self.apply(self.init_weights)

    def init_weights(self, m):
        """Initialize weights using the custom range from config.args."""
        if isinstance(m, nn.Linear):
            init.uniform_(m.weight, -config.args.init_weights, config.args.init_weights)
            if m.bias is not None:
                init.uniform_(m.bias, -config.args.init_weights, config.args.init_weights)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu1(x)
        x = self.dropout1(x)
        x = self.fc2(x)
        x = self.relu2(x)
        x = self.dropout2(x)
        x = self.fc3(x)
        return x