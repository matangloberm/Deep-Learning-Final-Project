# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1blrP6pX-hVT0d6f6352oZgZOyrJBnEB4
"""

from google.colab import drive
drive.mount('/content/drive')

import os
#!pip install argparse

folder_name = 'Final Project'
base_dir = "/content/drive/"
folder_path = None


for root, dirs, files in os.walk(base_dir):
    if folder_name in dirs:
        folder_path = os.path.join(root, folder_name)
        break

if folder_path is None:
    raise FileNotFoundError(f"Folder '{folder_name}' not found in Google Drive!")


os.chdir(folder_path)
print(f"✅ Working directory set to: {folder_path}")


!jupyter nbconvert --to python config2.ipynb --output=config2
!jupyter nbconvert --to python data_loader.ipynb --output=data_loader
!jupyter nbconvert --to python train.ipynb --output=train
!jupyter nbconvert --to python utils.ipynb --output=utils
!jupyter nbconvert --to python embedding_model.ipynb --output=embedding_model
!jupyter nbconvert --to python embedding_extractor.ipynb --output=embedding_extractor
!jupyter nbconvert --to python model.ipynb --output=model

import os

expected_files = ["config.py", "data_loader.py", "train.py", "utils.py", "embedding_model.py", "embedding_extractor.py"]

for file in expected_files:
    if os.path.exists(file):
        print(f"✅ {file} exists")
    else:
        print(f"❌ {file} is missing!")

import shutil

notebooks = ["config.py", "data_loader.py", "train.py", "utils.py", "embedding_model.py", "embedding_extractor.py"]

for nb in notebooks:
    src = f"/content/{nb}"  # Source path where `.py` files are generated
    dest = os.path.join(folder_path, nb)  # Destination inside the `Final Project` folder

    if os.path.exists(src):
        shutil.move(src, dest)
        print(f"✅ Moved {nb} to {folder_path}")


import sys
sys.path.append(folder_path)
print("✅ Project folder added to Python path")

if "config" in sys.modules:
    del sys.modules["config"]
import config2 as config
data_path = os.path.join(folder_path, "ArrangedData/data1")
print(f"✅ Data path set to: {data_path}")
model = "vits14"

config.args.embedding_model = model
config.args.data_path = data_path
config.args.model_save_path = os.path.join(folder_path, "saved_models", model, "with_augmentation_balanced") # specifies the folder of the trained fully connected model
config.args.save_path = os.path.join(folder_path, "saved_embeddings", model) # specifies the folder of the embedded samples
config.args.embedding_save_path = os.path.join(config.args.save_path, "embeddings_with_augmentation_balanced")
config.args.unzip_files =False
config.args.batch_size = 32


if config.args.embedding_model == "vitb14":
    config.args.hidden_size1 = 768
    config.args.hidden_size2 = 256

    config.args.dropout1 = 0.5
    config.args.dropout2 = 0.4

    config.args.init_weights = 0.05
    config.args.reg = 5e-4
    config.args.learning_rate = 5e-4

    config.args.epochs = 25

elif config.args.embedding_model == "vitl14":
    config.args.hidden_size1 = 768
    config.args.hidden_size2 = 256

    config.args.dropout1 = 0.3
    config.args.dropout2 = 0.2

    config.args.init_weights = 0.02  # Even lower for large models
    config.args.reg = 1e-4
    config.args.learning_rate = 2e-4

    config.args.epochs = 15
else:
    config.args.hidden_size1 = 256
    config.args.hidden_size2 = 64

    config.args.dropout1 = 0.6
    config.args.dropout2 = 0.5

    config.args.init_weights = 0.1
    config.args.reg = 1e-3
    config.args.learning_rate = 1e-3





#if not os.path.exists(config.args.save_path):
os.makedirs(config.args.save_path, exist_ok = True)
if os.path.exists(config.args.model_save_path) and not os.path.isdir(config.args.model_save_path):
    print(f"⚠️ ERROR: `{config.args.model_save_path}` exists but is NOT a directory! Deleting and recreating as a folder.")
    os.remove(config.args.model_save_path)  # ✅ Delete the misclassified file
    os.makedirs(config.args.model_save_path, exist_ok=True)  # ✅ Create as a proper directory
elif not os.path.exists(config.args.model_save_path):
  os.makedirs(config.args.model_save_path, exist_ok=True)  # ✅ Create normally if it doesn't exist


from data_loader import unzip_files, load_transformed_samples
from train import train_model, evaluate_model
import torch
from utils import set_random_seed
from embedding_model import upload_model

embedding_model = upload_model().to(config.args.device)
print(f"Updated dataset path: {config.args.data_path}")

# Set seed for reproducibility
set_random_seed(config.args.seed)
unzip_files(config.args.data_path + '/Training')
unzip_files(config.args.data_path + '/Test')
# Unzip datasets
if config.args.unzip_files:
  unzip_files(config.args.data_path + '/Training')
  unzip_files(config.args.data_path + '/Test')
else:
  print("Files unzipped, continue to load samples")

train_dataset = load_transformed_samples("Training", config.args.data_path, embedding_model)
test_dataset = load_transformed_samples("Test",config.args.data_path, embedding_model)
'''
if not os.path.exists(config.args.embedding_save_path):
  # Load and transform samples
  train_dataset = load_transformed_samples("Training", config.args.data_path, embedding_model)
  test_dataset = load_transformed_samples("Test",config.args.data_path, embedding_model)
else:
  train_path = os.path.join(config.args.embedding_save_path, "Training.pt")
  test_path = os.path.join(config.args.embedding_save_path, "Test.pt")
  train_val_dataset = torch.load(train_path) #dataset used for both validation and train
  test_dataset = torch.load(test_path)
'''

train_size = len(train_dataset) - 3
val_size = len(train_dataset) - train_size
train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])

# Convert to DataLoader
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=config.args.batch_size, shuffle=True)
val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=config.args.batch_size, shuffle=False)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=config.args.batch_size, shuffle=False)

print(f"✅ Training samples: {len(train_dataset)}")
print(f"✅ Validation samples: {len(val_dataset)}")
print(f"✅ Test samples: {len(test_dataset)}")

# Train and evaluate
train_model(train_loader, val_loader)
evaluate_model(test_loader)